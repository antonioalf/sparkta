MongoDB Specifications
******************

- :ref:`introduction-label`

- :ref:`driver-label`

- :ref:`worker-label`


.. _introduction-label:

Introduction
============

This output uses the native driver for MongoDB in Scala language. **Casbah Driver** offers all the functionality that
Sparkta need to insert,update and add and configure the database for proper system operation.

In the implementation of an Output of Sparkta's SDK there are two possibilities: one could be to allow transformations
 over data to a Spark's DataFrame. Other posibility could be to insert directly a **UpdateMetricOperation**. This
 output does not use any kind of Spark plugin to insert DataFrames.

This plugin creates one client connection per Worker in a Spark Cluster.

Is necessary need to override two functions from the Output SDK:
::
  override def doPersist(stream: DStream[(DimensionValuesTime, Map[String, Option[Any]])]): Unit
  override def upsert(metricOperations: Iterator[(DimensionValuesTime, Map[String, Option[Any]])]): Unit


.. _driver-label:

Driver Operations
============

When the driver starts the Output, several processes for creating indexes needed to run the various collections.
There are two types of indexes:

  * Unique index:

    Create primary key that contains a field called "id" values separated by "_" and the field timeDimension dimensions.
    This index is optional, because if you do not specify timeDimension not created and the primary key of the
    collection is "_id" field

  * Text index:

    If we do a text index on any dimension or some aggregate field, the system will create it for us.

      - Example:
      ::

        [
          {
            "v" : 1,
            "key" : {
              "_id" : 1
            },
            "name" : "_id_",
            "ns" : "sparkta.precision3_hashtags_retweets_minute"
          },
          {
            "v" : 1,
            "key" : {
              "_fts" : "text",
              "_ftsx" : 1
            },
            "name" : "userLocation",
            "ns" : "sparkta.precision3_hashtags_retweets_minute",
            "background" : true,
            "default_language" : "english",
            "weights" : {
              "userLocation" : 1
            },
            "language_override" : "language",
            "textIndexVersion" : 2
          },
          {
            "v" : 1,
            "unique" : true,
            "key" : {
              "id" : 1,
              "minute" : 1
            },
            "name" : "id_minute",
            "ns" : "sparkta.precision3_hashtags_retweets_minute",
            "background" : true
          }
        ]


.. _worker-label:

Worker Operations
============

As this Output does not use functionality of DataFrames, override the method Upsert, that save all values
of a **Tuple -> (DimensionValuesTime, Aggregations)**.
Below you can see each of the features implemented:

  * Each Worker save in one BulkOperation for each data partition of a RDD.

  * The output upsert documents with the _id field "dimension1_dimension2...". If timeDimension
    is specified in properties the system save the data in two fields "id" with the dimensions values and timeDimension
    field with the dateTime of the document. With the second the _id is autogenerated.

      - Example:
      ::

          "_id" : ObjectId("554891b3da00bdd0c284a573"),
          "id" : "List(0.703125, 0.703125)_1_0",
          "minute" : ISODate("2015-05-05T09:47:00Z"),
          "min_wordsN" : 1,
          "stddev_wordsN" : 2.8284271247461903,
          "avg_wordsN" : 6,
          "language" : "english",
          "variance_wordsN" : 8,
          "last_retweets" : NumberLong(0),
          "median_wordsN" : 6,
          "count" : NumberLong(750),
          "sum_wordsN" : NumberLong(7669),
          "max_wordsN" : 29,


  * MongoDB have several **Update Aggregation Commands** that are used by Sparkta for insert the aggregate fields. As
   can be Sum, Count, Avg, Max, Min ...
